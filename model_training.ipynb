{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a032a70-8a39-40d3-bb78-d9d701156927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.cluster import DBSCAN, KMeans, OPTICS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "class AbstractResult:\n",
    "    def __init__(self, name, X, features):\n",
    "        self.name = name\n",
    "        self.features = features\n",
    "        self.X = X\n",
    "        self.X_scaled = self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        print(f\"Preprocessing data for {self.name}\")\n",
    "        X_scaled = self.X[self.features]\n",
    "        # encode\n",
    "        label_encoder = LabelEncoder()\n",
    "        X_scaled[\"encoded_country\"] = label_encoder.fit_transform(self.X[\"country\"])\n",
    "        # scale\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_scaled) \n",
    "        print(f\"Preprocessing done...\")\n",
    "        return X_scaled\n",
    "\n",
    "    def show_overview(self):\n",
    "        print(f\"Model {self.name} overview\")\n",
    "        cluster_no = len(np.unique(self.labels))\n",
    "        if cluster_no == 1:\n",
    "            print(f\"Only one cluster found for {method}\")\n",
    "        else:\n",
    "            print(f\"Number of clusters: {cluster_no}\")\n",
    "        noise = self.X[self.labels == -1].shape[0]\n",
    "    \n",
    "        print(f\"Noise points: {noise}. Calculating metrics...\")\n",
    "    \n",
    "        no_noise_X = self.X_scaled[self.labels != -1]\n",
    "        no_noise_labels = self.labels[self.labels != -1]\n",
    "        #sil_score = silhouette_score(no_noise_X, no_noise_labels)\n",
    "        #print(f\"Silhouette Score: {sil_score}\")\n",
    "        ch_index = calinski_harabasz_score(no_noise_X, no_noise_labels)\n",
    "        print(f\"Calinski-Harabasz Index: {ch_index}\")\n",
    "        #db_index = davies_bouldin_score(no_noise_X, no_noise_labels)\n",
    "        #print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "\n",
    "        \n",
    "    def write_random_groups(self, proxy_folder=\"\"):\n",
    "        print(f\"Extracting random groups for {self.name}...\")\n",
    "        random_groups = np.random.choice(self.labels[self.labels != -1], size=40, replace=True)\n",
    "        path = self.name if proxy_folder == \"\" else f\"{self.name}/{proxy_folder}\"  \n",
    "        #for group in random_groups:\n",
    "        #    group_df = self.X[self.labels == group]\n",
    "        #    group_df.to_csv(f\"results/{path}/groups/group_{group}.csv\")\n",
    "        #print(\"Groups extracted!\")\n",
    "        pd.DataFrame(self.labels).to_csv(f\"results/{path}/labels.csv\")\n",
    "        print(\"Labels written.\")\n",
    "\n",
    "    \n",
    "        \n",
    "class ModelFramework(AbstractResult):\n",
    "    def __init__(self, name, X, features, model):\n",
    "        self.model = model \n",
    "        super().__init__(name, X, features)\n",
    "\n",
    "    def train(self, output_result=True):\n",
    "        print(f\"Start training {self.name}\")\n",
    "        self.labels = self.model.fit_predict(self.X_scaled)\n",
    "        print(f\"Training done\")\n",
    "        self.show_overview()\n",
    "        if output_result: \n",
    "            self.write_random_groups()\n",
    "\n",
    "class RandomisedKMeansRunner(AbstractResult):\n",
    "    def train(self, k_min=1000, k_max=20000, samples=20):\n",
    "        random_ks = random.sample(range(k_min, k_max+1), samples)\n",
    "        for k in random_ks:\n",
    "            print(f\"[NEW RUN] KMeans with {k} clusters.\")\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            self.labels = kmeans.fit_predict(self.X_scaled)\n",
    "            self.show_overview()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8a06d5-39eb-414b-b93e-663e67721fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for hdb_v1\n",
      "Preprocessing done...\n",
      "Preprocessing data for hdb_v2\n",
      "Preprocessing done...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"3_weeks.csv\")\n",
    "\n",
    "features_active_days = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'distinct_dest_ports', 'top_port', 'total_hits', 'distinct_ips', 'top_fingerprint', 'generation_algorithm']\n",
    "features_hours = ['subnet', 'asn', 'top_start_hour', 'top_end_hour', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'distinct_dest_ports', 'top_port', 'total_hits', 'distinct_ips', 'top_fingerprint', 'generation_algorithm'] \n",
    "features_quartiles = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'top_port', 'total_hits', 'distinct_ips', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip'] \n",
    "\n",
    "hdb_days = ModelFramework(\"hdb_v1\", df, features_active_days, hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True))\n",
    "hdb_hours = ModelFramework(\"hdb_v2\", df, features_hours, hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010fdd44-e6c9-4cbd-8c84-bbece8934d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for hdb_fs1\n",
      "Preprocessing done...\n",
      "Start training hdb_fs1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n",
      "Model hdb_fs1 overview\n",
      "Number of clusters: 115249\n",
      "Noise points: 316442. Calculating metrics...\n",
      "Calinski-Harabasz Index: 1171.744651069235\n",
      "Extracting random groups for hdb_fs1...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_hits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'total_hits', 'distinct_ips','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "hdb_fs1 = ModelFramework(\"hdb_fs1\", df, features_quartiles_hits, hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True))\n",
    "hdb_fs1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662567d8-9c87-46fd-ab78-a09edbdfdbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for hdb_fs2\n",
      "Preprocessing done...\n",
      "Start training hdb_fs2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n",
      "Model hdb_fs2 overview\n",
      "Number of clusters: 116333\n",
      "Noise points: 316265. Calculating metrics...\n",
      "Calinski-Harabasz Index: 1868.9511963100351\n",
      "Extracting random groups for hdb_fs2...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_nohits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "hdb_fs2 = ModelFramework(\"hdb_fs2\", df, features_quartiles_nohits, hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True))\n",
    "hdb_fs2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82210bc7-0933-425b-b66b-b2dbef7569e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon=0.5\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 10178\n",
      "Noise points: 53615. Calculating metrics...\n",
      "Calinski-Harabasz Index: 138.85137342364882\n",
      "Epsilon=0.55\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 8329\n",
      "Noise points: 43923. Calculating metrics...\n",
      "Calinski-Harabasz Index: 163.8052464182611\n",
      "Epsilon=0.6\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_hits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'total_hits', 'distinct_ips','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "for i in range(3):\n",
    "    epsilon = 0.5+0.05*i\n",
    "    print(f\"Epsilon={epsilon}\")\n",
    "    db = ModelFramework(\"db_eps_test\", df, features_quartiles_hits, DBSCAN(eps=epsilon, min_samples=2))\n",
    "    db.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fe3b90-513d-4861-a09a-2675eacc959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon=0.05\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 48031\n",
      "Noise points: 469659. Calculating metrics...\n",
      "Calinski-Harabasz Index: 391.3572604852084\n",
      "Epsilon=0.1\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 48082\n",
      "Noise points: 360209. Calculating metrics...\n",
      "Calinski-Harabasz Index: 281.17330421181595\n",
      "Epsilon=0.15000000000000002\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 43385\n",
      "Noise points: 280721. Calculating metrics...\n",
      "Calinski-Harabasz Index: 173.97649081251697\n",
      "Epsilon=0.2\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 38360\n",
      "Noise points: 220419. Calculating metrics...\n",
      "Calinski-Harabasz Index: 103.42397301553888\n",
      "Epsilon=0.25\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 31897\n",
      "Noise points: 174229. Calculating metrics...\n",
      "Calinski-Harabasz Index: 78.88676630277116\n",
      "Epsilon=0.3\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 23288\n",
      "Noise points: 132761. Calculating metrics...\n",
      "Calinski-Harabasz Index: 79.50548295483321\n",
      "Epsilon=0.35000000000000003\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 18186\n",
      "Noise points: 101403. Calculating metrics...\n",
      "Calinski-Harabasz Index: 87.51080487394248\n",
      "Epsilon=0.4\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 14797\n",
      "Noise points: 79674. Calculating metrics...\n",
      "Calinski-Harabasz Index: 98.53820172115266\n",
      "Epsilon=0.45\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 12019\n",
      "Noise points: 62954. Calculating metrics...\n",
      "Calinski-Harabasz Index: 114.48553944163673\n",
      "Epsilon=0.5\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 9807\n",
      "Noise points: 51006. Calculating metrics...\n",
      "Calinski-Harabasz Index: 133.2566361548991\n",
      "Epsilon=0.55\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 7993\n",
      "Noise points: 41550. Calculating metrics...\n",
      "Calinski-Harabasz Index: 156.46976273211362\n",
      "Epsilon=0.6000000000000001\n",
      "Preprocessing data for db_eps_test\n",
      "Preprocessing done...\n",
      "Start training db_eps_test\n",
      "Training done\n",
      "Model db_eps_test overview\n",
      "Number of clusters: 6346\n",
      "Noise points: 33994. Calculating metrics...\n",
      "Calinski-Harabasz Index: 185.93996630547448\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_nohits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "for i in range(12):\n",
    "    epsilon = 0.05+0.05*i\n",
    "    print(f\"Epsilon={epsilon}\")\n",
    "    db = ModelFramework(\"db_eps_test\", df, features_quartiles_nohits, DBSCAN(eps=epsilon, min_samples=2))\n",
    "    db.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251d65c4-b43d-428b-88c2-28e81b3a4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for db_fs1\n",
      "Preprocessing done...\n",
      "Start training db_fs1\n",
      "Training done\n",
      "Model db_fs1 overview\n",
      "Number of clusters: 15092\n",
      "Noise points: 82745. Calculating metrics...\n",
      "Calinski-Harabasz Index: 103.18848580282702\n",
      "Extracting random groups for db_fs1...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_hits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'total_hits', 'distinct_ips','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "db = ModelFramework(\"db_fs1\", df, features_quartiles_hits, DBSCAN(eps=0.4, min_samples=2))\n",
    "db.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64855fd0-06bd-49e1-9221-a73345388337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for db_fs2\n",
      "Preprocessing done...\n",
      "Start training db_fs2\n",
      "Training done\n",
      "Model db_fs2 overview\n",
      "Number of clusters: 14797\n",
      "Noise points: 79674. Calculating metrics...\n",
      "Calinski-Harabasz Index: 98.53820172115266\n",
      "Extracting random groups for db_fs2...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_nohits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "db = ModelFramework(\"db_fs2\", df, features_quartiles_nohits, DBSCAN(eps=0.4, min_samples=2))\n",
    "db.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a5eb67-d5dd-43e9-a59b-2db4d418e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for hdb_fs1_10pts\n",
      "Preprocessing done...\n",
      "Start training hdb_fs1_10pts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n",
      "Model hdb_fs1_10pts overview\n",
      "Number of clusters: 10398\n",
      "Noise points: 493000. Calculating metrics...\n",
      "Calinski-Harabasz Index: 3649.642678757402\n",
      "Extracting random groups for hdb_fs1_10pts...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_hits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'total_hits', 'distinct_ips','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "hdb_fs1_10pts = ModelFramework(\"hdb_fs1_10pts\", df, features_quartiles_hits, hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True))\n",
    "hdb_fs1_10pts.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdf2509-888c-4e91-89fa-cf8ab05c64e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for hdb_fs2_10pts\n",
      "Preprocessing done...\n",
      "Start training hdb_fs2_10pts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n",
      "Model hdb_fs2_10pts overview\n",
      "Number of clusters: 10440\n",
      "Noise points: 493702. Calculating metrics...\n",
      "Calinski-Harabasz Index: 3671.302359745206\n",
      "Extracting random groups for hdb_fs2_10pts...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_nohits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "hdb_fs2_10pts = ModelFramework(\"hdb_fs2_10pts\", df, features_quartiles_nohits, hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True))\n",
    "hdb_fs2_10pts.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17320da-d365-48d3-b6ad-d79cc48e62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for db_fs1_eps01\n",
      "Preprocessing done...\n",
      "Start training db_fs1_eps01\n",
      "Training done\n",
      "Model db_fs1_eps01 overview\n",
      "Number of clusters: 47770\n",
      "Noise points: 362851. Calculating metrics...\n",
      "Calinski-Harabasz Index: 294.4206785244829\n",
      "Extracting random groups for db_fs1_eps01...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_hits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports', 'total_hits', 'distinct_ips','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "db_fs1_eps01 = ModelFramework(\"db_fs1_eps01\", df, features_quartiles_hits, DBSCAN(eps=0.1, min_samples=2))\n",
    "db_fs1_eps01.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4acf1840-1562-4396-b8d6-263a5f859b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for db_fs2_eps01\n",
      "Preprocessing done...\n",
      "Start training db_fs2_eps01\n",
      "Training done\n",
      "Model db_fs2_eps01 overview\n",
      "Number of clusters: 48082\n",
      "Noise points: 360209. Calculating metrics...\n",
      "Calinski-Harabasz Index: 281.17330421181595\n",
      "Extracting random groups for db_fs2_eps01...\n",
      "Labels written.\n"
     ]
    }
   ],
   "source": [
    "features_quartiles_nohits = ['subnet', 'asn', 'active_days', 'scan_length_seconds', 'median_time_diff', 'distinct_src_ports', 'avg_payload_length', 'distinct_dest_ports','top_port', 'top_fingerprint', 'q1_prev_ip', 'median_prev_ip', 'q3_prev_ip']\n",
    "db_fs2_eps01 = ModelFramework(\"db_fs2_eps01\", df, features_quartiles_nohits, DBSCAN(eps=0.1, min_samples=2))\n",
    "db_fs2_eps01.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac94d50-fea4-4d8a-b08f-49bd49684682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
